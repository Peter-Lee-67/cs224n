{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-10-04T23:06:51.611105Z",
     "end_time": "2023-10-04T23:06:57.523313Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19., 20., 21.],\n",
      "        [22., 23., 24., 25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32., 33., 34., 35.]])\n",
      "Taking the sum over columns:\n",
      "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
      "Taking thep sum over rows:\n",
      "tensor([ 28.,  77., 126., 175., 224.])\n",
      "Taking the sum of all:\n",
      "tensor(630.)\n",
      "Taking the stdev over rows:\n",
      "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"
     ]
    }
   ],
   "source": [
    "# Vectorized operation\n",
    "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
    "print(\"Data is:\", data)\n",
    "\n",
    "print(\"Taking the sum over columns:\")\n",
    "print(data.sum(dim=0))\n",
    "\n",
    "print(\"Taking thep sum over rows:\")\n",
    "print(data.sum(dim=1))\n",
    "\n",
    "print(\"Taking the sum of all:\")\n",
    "print(data.sum())\n",
    "\n",
    "print(\"Taking the stdev over rows:\")\n",
    "print(data.std(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T19:52:04.503398Z",
     "end_time": "2023-10-04T19:52:04.581543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([4.2667, 1.0333])\n",
      "torch.Size([3])\n",
      "tensor([ 2.5000, -2.5000,  7.9500])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([[1, 2.2, 9.6],[4, -7.2, 6.3]])\n",
    "row_avg = data.mean(dim=1)\n",
    "col_avg = data.mean(dim=0)\n",
    "\n",
    "print(row_avg.shape)\n",
    "print(row_avg)\n",
    "\n",
    "print(col_avg.shape)\n",
    "print(col_avg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T19:52:56.650552Z",
     "end_time": "2023-10-04T19:52:56.702198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.,  2.],\n          [ 3.,  4.]],\n \n         [[ 5.,  6.],\n          [ 7.,  8.]],\n \n         [[ 9., 10.],\n          [11., 12.]]]),\n torch.Size([3, 2, 2]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]],\n",
    "                  [[9, 10], [11, 12]]\n",
    "                 ])\n",
    "x, x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T19:54:32.759876Z",
     "end_time": "2023-10-04T19:54:32.775736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2.],\n        [3., 4.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T19:55:06.417311Z",
     "end_time": "2023-10-04T19:55:06.427390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.,  2.],\n        [ 5.,  6.],\n        [ 9., 10.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0] # shape:(3， 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:01:13.304635Z",
     "end_time": "2023-10-04T20:01:13.321367Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "matr = torch.arange(1, 16).view(5, 3)\n",
    "print(matr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:02:28.285939Z",
     "end_time": "2023-10-04T20:02:28.297079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2, 3])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0] # shape:(3, )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:02:47.276269Z",
     "end_time": "2023-10-04T20:02:47.299614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1,  4,  7, 10, 13])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:03:46.009589Z",
     "end_time": "2023-10-04T20:03:46.056811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:04:20.696546Z",
     "end_time": "2023-10-04T20:04:20.727798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2],\n        [ 4,  5],\n        [ 7,  8],\n        [10, 11],\n        [13, 14]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[:, 0:2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:04:30.344888Z",
     "end_time": "2023-10-04T20:04:30.351906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [4, 5],\n        [7, 8]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3, 0:2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:04:47.569083Z",
     "end_time": "2023-10-04T20:04:47.617553Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:05:00.053003Z",
     "end_time": "2023-10-04T20:05:00.084756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 6, 9])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3, 2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:05:13.988531Z",
     "end_time": "2023-10-04T20:05:13.995552Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3],\n         [ 4,  5,  6],\n         [ 7,  8,  9],\n         [10, 11, 12],\n         [13, 14, 15]]),\n tensor([7, 8, 9]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr, matr[0:3][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:06:17.352476Z",
     "end_time": "2023-10-04T20:06:17.371909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:07:40.087709Z",
     "end_time": "2023-10-04T20:07:40.093218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3],\n        [ 7,  8,  9],\n        [13, 14, 15]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[[0, 2, 4]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:08:00.466654Z",
     "end_time": "2023-10-04T20:08:00.513776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.,  2.],\n          [ 3.,  4.]],\n \n         [[ 5.,  6.],\n          [ 7.,  8.]],\n \n         [[ 9., 10.],\n          [11., 12.]]]),\n tensor([1., 5., 9.]))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x[:, 0, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:08:26.989426Z",
     "end_time": "2023-10-04T20:08:27.021174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.,  2.],\n         [ 3.,  4.]],\n\n        [[ 5.,  6.],\n         [ 7.,  8.]],\n\n        [[ 9., 10.],\n         [11., 12.]]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:09:06.182637Z",
     "end_time": "2023-10-04T20:09:06.239053Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.,  2.],\n          [ 3.,  4.]],\n \n         [[ 5.,  6.],\n          [ 7.,  8.]],\n \n         [[ 9., 10.],\n          [11., 12.]]]),\n tensor([[[True, True],\n          [True, True]],\n \n         [[True, True],\n          [True, True]],\n \n         [[True, True],\n          [True, True]],\n \n         [[True, True],\n          [True, True]]]))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([0, 0, 1, 1])\n",
    "x, x[i], x[[0, 0, 1, 1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:12:00.737186Z",
     "end_time": "2023-10-04T20:12:00.753235Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.,  2.],\n          [ 3.,  4.]],\n \n         [[ 5.,  6.],\n          [ 7.,  8.]],\n \n         [[ 9., 10.],\n          [11., 12.]]]),\n tensor([[ 5.,  6.],\n         [ 9., 10.]]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([1, 2])\n",
    "j = torch.tensor([0])\n",
    "x, x[i, j]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:13:03.382890Z",
     "end_time": "2023-10-04T20:13:03.426137Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:14:48.163661Z",
     "end_time": "2023-10-04T20:14:48.231936Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0].item()   # scalar value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:14:53.630686Z",
     "end_time": "2023-10-04T20:14:53.661973Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 4.]), tensor([1.0000, 2.2000, 9.6000]))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise\n",
    "data = torch.tensor([[1, 2.2, 9.6], [4, -7.2, 6.3]])\n",
    "first_col = data[:, 0]\n",
    "first_row = data[0]\n",
    "first_col, first_row"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:17:02.824518Z",
     "end_time": "2023-10-04T20:17:02.895101Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import pprint as pp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:20:02.055444Z",
     "end_time": "2023-10-04T20:20:02.087197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:20:02.671128Z",
     "end_time": "2023-10-04T20:20:02.733035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "y = x * x * 3 # 3x^2\n",
    "y.backward()\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:20:25.755975Z",
     "end_time": "2023-10-04T20:20:25.802918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:20:53.829192Z",
     "end_time": "2023-10-04T20:20:53.835256Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:22:30.122948Z",
     "end_time": "2023-10-04T20:22:30.169876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3955, -1.1012],\n         [ 0.3955, -1.1012],\n         [ 0.3955, -1.1012]],\n\n        [[ 0.3955, -1.1012],\n         [ 0.3955, -1.1012],\n         [ 0.3955, -1.1012]]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(2,3,4)\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(input)\n",
    "linear_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:28:19.489748Z",
     "end_time": "2023-10-04T20:28:19.592750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.2948,  0.2432,  0.1184,  0.3072],\n         [ 0.3915, -0.3828, -0.2548, -0.3784]], requires_grad=True),\n Parameter containing:\n tensor([ 0.0215, -0.4766], requires_grad=True)]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear.parameters())   # Ax + b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:28:46.056262Z",
     "end_time": "2023-10-04T20:28:46.150241Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nData of shape [batch_size, feature_dim] # 4\\n[batch_size, output_dim] # 2\\nlinear layer of shape (feature_dim, output_dim)\\n'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data of shape [batch_size, feature_dim] # 4\n",
    "[batch_size, output_dim] # 2\n",
    "linear layer of shape (feature_dim, output_dim)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:29:51.010071Z",
     "end_time": "2023-10-04T20:29:51.072013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3955, -1.1012],\n         [ 0.3955, -1.1012],\n         [ 0.3955, -1.1012]],\n\n        [[ 0.3955, -1.1012],\n         [ 0.3955, -1.1012],\n         [ 0.3955, -1.1012]]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:32:24.933952Z",
     "end_time": "2023-10-04T20:32:24.965324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.5976, 0.2495],\n         [0.5976, 0.2495],\n         [0.5976, 0.2495]],\n\n        [[0.5976, 0.2495],\n         [0.5976, 0.2495],\n         [0.5976, 0.2495]]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:35:57.345651Z",
     "end_time": "2023-10-04T20:35:57.408279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.7427, 0.5655],\n         [0.7427, 0.5655],\n         [0.7427, 0.5655]],\n\n        [[0.7427, 0.5655],\n         [0.7427, 0.5655],\n         [0.7427, 0.5655]]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "input = torch.ones(2,3,4)\n",
    "output = block(input)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:38:13.557218Z",
     "end_time": "2023-10-04T20:38:13.651483Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "自定义模块\n",
    "我们可以通过扩展nn.Module类构建属于我们自己的模块，而不是使用预定义好的模块。\n",
    "举个例子，我们可以用之前介绍过的张量构建一个自己的nn.Linear。\n",
    "我们也可以构建一个新的，更加复杂的模块，例如一个自定义的神经网络。\n",
    "你将在后面的作业中练习这些内容。\n",
    "为了创建一个自定义模块，我们必须要做的第一件事是扩展nn.Module。\n",
    "我们可以在__init__函数中初始化我们的参数，这通过在一开始调用超类的__init__函数\n",
    "完成。我们定义的所有属于nn模块对象的类属性被视为在训练中可学习的参数。张量不是参数，\n",
    "但是当它们封装在nn.Parameter类当中时可以转化为参数。\n",
    "所有扩展nn.Module的类最好能够实现forward(x)函数，其中x是一个张量。\n",
    "这是一个在参数传递到我们模块当中时调用的函数，比如在使用model(x)时。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:55:36.905942Z",
     "end_time": "2023-10-04T20:55:36.921885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3805, 0.4069, 0.4430, 0.4944, 0.4828],\n        [0.3970, 0.4144, 0.4003, 0.4809, 0.4570]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 5)\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "model(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:55:38.818653Z",
     "end_time": "2023-10-04T20:55:38.874531Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "[('model.0.weight',\n  Parameter containing:\n  tensor([[-0.3164, -0.3948,  0.0007,  0.4110, -0.1013],\n          [ 0.1920, -0.3115, -0.1146, -0.2034, -0.2211],\n          [ 0.2331, -0.0947,  0.2937,  0.4448,  0.3240]], requires_grad=True)),\n ('model.0.bias',\n  Parameter containing:\n  tensor([ 0.1063, -0.1179,  0.3035], requires_grad=True)),\n ('model.2.weight',\n  Parameter containing:\n  tensor([[ 0.1402,  0.4693, -0.1998],\n          [ 0.0039, -0.0693, -0.3902],\n          [-0.3732,  0.0601,  0.4037],\n          [-0.1194,  0.1520,  0.1026],\n          [-0.3807,  0.3686, -0.5746]], requires_grad=True)),\n ('model.2.bias',\n  Parameter containing:\n  tensor([-0.4371, -0.2789, -0.3306, -0.0484,  0.0755], requires_grad=True))]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:55:49.644184Z",
     "end_time": "2023-10-04T20:55:49.708274Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.3164, -0.3948,  0.0007,  0.4110, -0.1013],\n         [ 0.1920, -0.3115, -0.1146, -0.2034, -0.2211],\n         [ 0.2331, -0.0947,  0.2937,  0.4448,  0.3240]], requires_grad=True),\n Parameter containing:\n tensor([ 0.1063, -0.1179,  0.3035], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1402,  0.4693, -0.1998],\n         [ 0.0039, -0.0693, -0.3902],\n         [-0.3732,  0.0601,  0.4037],\n         [-0.1194,  0.1520,  0.1026],\n         [-0.3807,  0.3686, -0.5746]], requires_grad=True),\n Parameter containing:\n tensor([-0.4371, -0.2789, -0.3306, -0.0484,  0.0755], requires_grad=True)]"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T20:56:12.325987Z",
     "end_time": "2023-10-04T20:56:12.388502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:04:06.922080Z",
     "end_time": "2023-10-04T21:04:06.950779Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.2668,  1.7910,  1.5346, -0.4032,  0.3213],\n        [ 0.7083,  1.7907,  2.0176, -0.2192,  0.8725],\n        [ 0.6034,  0.8095, -0.0816, -0.5976,  2.6324],\n        [ 2.3761,  0.1692,  0.8731,  0.4185,  1.0365],\n        [ 0.7275,  1.8190,  1.8786,  0.4107,  1.1052],\n        [ 1.6249,  0.1531, -0.2657,  0.7569,  1.3995],\n        [-0.1918,  2.6969,  0.8087,  0.8950,  0.1490],\n        [ 1.0754,  1.1609,  0.2950,  1.1435,  2.4459],\n        [ 0.5117, -0.6529,  0.4466, -0.3124,  0.4715],\n        [ 1.0038, -0.4121,  2.8416,  2.2504, -0.0415]])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(10, 5)\n",
    "x = y + torch.randn_like(y)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:05:15.543735Z",
     "end_time": "2023-10-04T21:05:15.596839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6932042837142944"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(5, 3)\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "loss_function = nn.BCELoss()\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:07:27.561791Z",
     "end_time": "2023-10-04T21:07:27.608707Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: traing loss: 0.6932042837142944\n",
      "Epoch 1: traing loss: 0.5639248490333557\n",
      "Epoch 2: traing loss: 0.448487788438797\n",
      "Epoch 3: traing loss: 0.34124821424484253\n",
      "Epoch 4: traing loss: 0.24731726944446564\n",
      "Epoch 5: traing loss: 0.1686665117740631\n",
      "Epoch 6: traing loss: 0.10722093284130096\n",
      "Epoch 7: traing loss: 0.06405559182167053\n",
      "Epoch 8: traing loss: 0.0371493324637413\n",
      "Epoch 9: traing loss: 0.021797971799969673\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    adam.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
    "    loss.backward()\n",
    "    adam.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:09:48.689971Z",
     "end_time": "2023-10-04T21:09:48.770486Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.5380, -0.3665, -0.6788, -0.0559, -0.1045],\n         [ 0.8991,  1.1448,  1.3504,  0.9494,  1.1555],\n         [ 0.3568, -0.0857, -0.2778, -0.0253, -0.3758]], requires_grad=True),\n Parameter containing:\n tensor([-0.1227,  1.4232, -0.3917], requires_grad=True),\n Parameter containing:\n tensor([[-0.1119,  1.3390, -0.4672],\n         [ 0.8958,  0.5136, -0.4674],\n         [ 0.0234,  1.1834,  0.2619],\n         [ 0.8486,  1.3662,  0.2864],\n         [ 0.1000,  0.7296, -0.5668]], requires_grad=True),\n Parameter containing:\n tensor([0.6550, 0.4905, 0.4034, 0.9733, 1.2370], requires_grad=True)]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:09:55.207978Z",
     "end_time": "2023-10-04T21:09:55.241112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9986, 0.9398, 0.9963, 0.9991, 0.9884],\n        [1.0000, 0.9880, 0.9999, 1.0000, 0.9989],\n        [0.9995, 0.9605, 0.9987, 0.9997, 0.9938],\n        [0.9999, 0.9790, 0.9997, 0.9999, 0.9975],\n        [1.0000, 0.9917, 1.0000, 1.0000, 0.9994],\n        [0.9994, 0.9559, 0.9983, 0.9996, 0.9927],\n        [0.9999, 0.9782, 0.9997, 0.9999, 0.9974],\n        [1.0000, 0.9902, 1.0000, 1.0000, 0.9992],\n        [0.9650, 0.8192, 0.9402, 0.9756, 0.9362],\n        [1.0000, 0.9889, 0.9999, 1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T21:10:15.128432Z",
     "end_time": "2023-10-04T21:10:15.144070Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demo: Word Window Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:47.449779Z",
     "end_time": "2023-10-05T23:36:47.499777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[['we', 'always', 'come', 'to', 'paris'],\n ['the', 'professor', 'is', 'from', 'australia'],\n ['i', 'live', 'in', 'stanford'],\n ['he', 'comes', 'from', 'taiwan'],\n ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单词小写化\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:49.704896Z",
     "end_time": "2023-10-05T23:36:49.888891Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于每一个训练样本，我们应该有一个相应的标签。\n",
    "还记得我们模型的目标是决定哪些词对应一个地点(LOCATION)。\n",
    "换言之，我们想要我们的模型对所有不是地点的词输出0，而\n",
    "对所有是地点的词输出1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0, 0, 0, 0, 1],\n [0, 0, 0, 0, 1],\n [0, 0, 0, 1],\n [0, 0, 0, 1],\n [0, 0, 0, 1, 0, 1]]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:51.784894Z",
     "end_time": "2023-10-05T23:36:51.865891Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将单词转换成向量\n",
    "让我们进一步考察我们的训练数据。我们拥有的每一个数据点是一个单词的序列。\n",
    "另一方面，我们知道机器学习模型处理的是向量中的数字。\n",
    "那我们如何将单词转换成数字呢？你可能想到了词嵌入，没错你是对的！\n",
    "假设我们有一个词嵌入的查找表E，E的每一行对应一个词嵌入。\n",
    "因此在词汇表中的每一个单词在E中都有一个对应的嵌入行i。\n",
    "每当我们想要为一个单词找到对应的嵌入时，我们将采用这些步骤：\n",
    "1.word -> index\n",
    "2.index -> embedding\n",
    "让我们看看第一步。我们应该为词汇表中的所有单词分配一个对应的索引。\n",
    "我们能做以下事情：\n",
    "1.找到语料中所有去重后的词\n",
    "2.为每个词分配索引"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'always',\n 'ankara',\n 'australia',\n 'capital',\n 'come',\n 'comes',\n 'from',\n 'he',\n 'i',\n 'in',\n 'is',\n 'live',\n 'of',\n 'paris',\n 'professor',\n 'stanford',\n 'taiwan',\n 'the',\n 'to',\n 'turkey',\n 'we'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set([w for s in train_sentences for w in s])\n",
    "vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:53.412411Z",
     "end_time": "2023-10-05T23:36:53.462409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 词汇表中添加未知词\"<unk>\"\n",
    "vocabulary.add(\"<unk>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:54.179920Z",
     "end_time": "2023-10-05T23:36:54.206923Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表中添加填充词\"<pad>\"\n",
    "# 两端填充\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:55.064920Z",
     "end_time": "2023-10-05T23:36:55.098924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'<pad>',\n '<unk>',\n 'always',\n 'ankara',\n 'australia',\n 'capital',\n 'come',\n 'comes',\n 'from',\n 'he',\n 'i',\n 'in',\n 'is',\n 'live',\n 'of',\n 'paris',\n 'professor',\n 'stanford',\n 'taiwan',\n 'the',\n 'to',\n 'turkey',\n 'we'}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:55.659918Z",
     "end_time": "2023-10-05T23:36:55.706921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'<pad>': 0,\n '<unk>': 1,\n 'always': 2,\n 'ankara': 3,\n 'australia': 4,\n 'capital': 5,\n 'come': 6,\n 'comes': 7,\n 'from': 8,\n 'he': 9,\n 'i': 10,\n 'in': 11,\n 'is': 12,\n 'live': 13,\n 'of': 14,\n 'paris': 15,\n 'professor': 16,\n 'stanford': 17,\n 'taiwan': 18,\n 'the': 19,\n 'to': 20,\n 'turkey': 21,\n 'we': 22}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引到单词：列表\n",
    "# 单词到索引：字典\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "word_to_ix = {word:ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:56.362253Z",
     "end_time": "2023-10-05T23:36:56.398996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'we' in word_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:57.042081Z",
     "end_time": "2023-10-05T23:36:57.069261Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们已经准备好将我们的训练句子转换成一个与单词对应的索引的序列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 给定一个句子，返回对应的索引\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        if token in word_to_ix:\n",
    "            indices.append(word_to_ix[token])\n",
    "        else:\n",
    "            indices.append(word_to_ix['<unk>'])\n",
    "    return indices\n",
    "\n",
    "# convert_token_to_indices的紧凑形式(?)\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "    \"\"\"\n",
    "    dictionary.get(key, default)\n",
    "    用于从字典中获取指定键的值，当key不存在于字典中时，返回default\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:58.244099Z",
     "end_time": "2023-10-05T23:36:58.286097Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:58.831097Z",
     "end_time": "2023-10-05T23:36:58.855096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[[22, 2, 6, 20, 15],\n [19, 16, 12, 8, 4],\n [10, 13, 11, 17],\n [9, 7, 8, 18],\n [19, 5, 14, 21, 12, 3]]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:36:59.269815Z",
     "end_time": "2023-10-05T23:36:59.349855Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以用PyTorch中的nn.Embedding类创建一个嵌入表。\n",
    "nn.Embedding(num_words, embedding_dimension)，其中num_words是\n",
    "词汇表中的单词数，embedding_dimension是我们希望的词嵌入维数。\n",
    "当训练网络时，梯度会反向一路传播到embedding层，因此我们的词嵌入将得到更新。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-4.6027e-01, -8.4919e-01, -1.1172e+00,  4.7096e-01, -1.1386e+00],\n         [ 9.7457e-01, -4.7157e-02, -1.9248e-01, -8.8030e-01,  1.7610e+00],\n         [-8.8988e-01, -1.0015e+00,  2.2139e+00, -2.8757e-01, -4.0670e-03],\n         [ 1.1170e+00, -2.6294e-01,  8.4020e-01,  1.2682e+00, -9.3508e-01],\n         [ 1.9339e+00,  8.6110e-01,  5.9314e-01, -4.8939e-01, -6.1035e-01],\n         [ 3.4974e-01, -2.1238e+00, -4.9988e-01, -7.8539e-01,  1.1435e-01],\n         [ 1.1516e-01,  5.0008e-01, -7.3795e-01, -1.3123e+00,  1.8227e-01],\n         [ 1.3435e+00, -3.7310e-01,  4.7257e-01, -8.9900e-01,  1.1649e+00],\n         [-4.2022e-01,  5.9605e-01,  8.5568e-01,  5.5407e-01, -1.0130e-01],\n         [-5.8360e-01,  8.9255e-01,  7.5948e-02, -1.2467e+00,  1.0256e+00],\n         [-2.0083e+00,  1.7923e+00, -2.2006e+00,  5.6028e-01, -6.5850e-01],\n         [ 1.8532e+00, -9.1294e-01,  8.7202e-01, -6.1887e-01, -1.8428e+00],\n         [-4.5302e-01, -1.2417e-01, -9.4466e-01,  1.9372e+00,  8.5324e-01],\n         [ 9.4668e-01,  7.0161e-01, -5.3838e-01, -2.2486e-01,  1.8017e-01],\n         [-5.8675e-01, -5.7252e-02, -1.2935e-01,  8.9894e-01,  1.0114e+00],\n         [-1.6033e+00,  9.3938e-02, -5.0169e-01,  1.4697e+00, -7.6367e-01],\n         [ 4.3781e-01, -6.7096e-02, -2.2522e+00, -1.3446e+00, -3.9933e-01],\n         [-1.5107e+00, -5.7408e-01, -3.6572e-02, -2.5770e-01,  5.9798e-01],\n         [-1.7585e+00,  9.1367e-01, -4.9211e-01, -6.1833e-01,  6.9469e-01],\n         [-5.9356e-01, -4.0138e-02,  9.6729e-01,  5.4427e-01,  3.0187e-01],\n         [-9.5705e-02,  1.7577e+00, -2.0237e-02, -1.1827e+00, -1.0261e-01],\n         [ 6.0387e-04, -1.1449e+00,  1.4301e+00, -5.1298e-01,  2.2658e+00],\n         [-4.8993e-01, -7.5509e-02,  6.4064e-01, -6.7040e-01, -7.3580e-02]],\n        requires_grad=True)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词向量初始化\n",
    "import torch.nn as nn\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "list(embeds.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:00.347247Z",
     "end_time": "2023-10-05T23:37:06.540171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-1.6033,  0.0939, -0.5017,  1.4697, -0.7637],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回单个给定词的词向量\n",
    "import torch\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:06.543168Z",
     "end_time": "2023-10-05T23:37:06.599208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.6033,  0.0939, -0.5017,  1.4697, -0.7637],\n        [ 1.1170, -0.2629,  0.8402,  1.2682, -0.9351]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回多个给定词(列表)的词向量\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:06.573168Z",
     "end_time": "2023-10-05T23:37:06.599208Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以写一个自定义函数并传递给collate_fn，以此打印批量的状态或者进行额外的处理。\n",
    "在我们的例子中，我们将使用collate_fn：\n",
    "1.用\"<pad>\"填充训练的句子\n",
    "2.将训练样本中的单词转换成索引\n",
    "3.将所有句子和标签填充到相同长度\n",
    "当计算损失时，需要知道给定样本中确切的单词数，我们将在传递给collate_fn的函数中追踪\n",
    "这个单词数。\n",
    "在collate_fn函数中需要用到word_to_ix使单词转化成索引，这里使用了python中的\n",
    "partial函数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# DataLoader + 数据预处理\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    x, y = zip(*batch)\n",
    "    \"\"\"\n",
    "    zip用于将多个可迭代对象(列表，元组，集合等)按照位置一一配对，接受一个\n",
    "    或多个可迭代对象，返回迭代器，该迭代器包含输入可迭代对象中相同位置元素\n",
    "    的元组\n",
    "    zip的用法之一是解压元组：\n",
    "    pairs = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "    numbers, letters = zip(*pairs)\n",
    "    print(numbers) # -> (1,2,3)\n",
    "    print(letters) # -> ('a','b','c')\n",
    "    \"\"\"\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "\n",
    "    # 在两端用<pad>填充训练样本\n",
    "    x = [pad_window(s, window_size) for s in x]\n",
    "\n",
    "    # 将训练样本转换成索引\n",
    "    def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "    x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "    # 填充样本使得一个批量中的所有样本长度相同，便于完成矩阵运算\n",
    "    # 我们将batch_first参数设置为True，由此返回的矩阵的第一维是批量\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "    # pad_sequence函数期望输入是张量，因此先转成LongTensor\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True,padding_value=pad_token_ix)    # 填充后的一致长度我们暂时不知\n",
    "\n",
    "    # 我们还需要填充标签，在此之前，我们将记录标签的数量以便知道每个样本中的单词数\n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return x_padded, y_padded, lenghts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:10.004083Z",
     "end_time": "2023-10-05T23:37:10.048954Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 无注释精简版\n",
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    x, y = zip(*batch)\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "    x = [pad_window(s, window_size) for s in x]\n",
    "    def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "    x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=word_to_ix[\"<pad>\"])\n",
    "\n",
    "    # 没有对y进行pad_window\n",
    "    # 无需对y进行convert_tokens_to_indices\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    lengths = [len(label) for label in y]\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    return x_padded, y_padded, lengths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:10.938956Z",
     "end_time": "2023-10-05T23:37:10.949952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([5, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n",
      "        [ 0,  0,  9,  7,  8, 18,  0,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0]])\n",
      "Batched Lengths:\n",
      "tensor([6, 4])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\"\"\"\n",
    "functools.partial用于创建一个新函数，该函数对现有函数的部分参数\n",
    "进行了预先设定，使这些参数在创建'partial'对象时就已经被设置\n",
    "举例：\n",
    "from functools import partial\n",
    "add_five = partial(lambda x, y: x + y, 5)\n",
    "result = add_five(10) # -> 15\n",
    "\n",
    "join_words = partial(str.join, sep = ' ')\n",
    "result = join_words(['hello', 'world']) # -> 'hello world'\n",
    "\"\"\"\n",
    "\n",
    "loader = DataLoader(data, batch_size=batch_size,shuffle=shuffle,collate_fn=collate_fn)\n",
    "# 遍历一个epoch\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(\"Batched Input:\")\n",
    "    print(batched_x)\n",
    "    print(\"Batched Labels:\")\n",
    "    print(batched_y)\n",
    "    print(\"Batched Lengths:\")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:11.645079Z",
     "end_time": "2023-10-05T23:37:11.678078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 22,  2,  6],\n",
      "         [ 0, 22,  2,  6, 20],\n",
      "         [22,  2,  6, 20, 15],\n",
      "         [ 2,  6, 20, 15,  0],\n",
      "         [ 6, 20, 15,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# 由原始序列返回多窗口\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "\"\"\"\n",
    "unfold(dimension, size, step)\n",
    "用于对张量进行滑动窗口操作\n",
    "dim: 滑动窗口的维度(轴)索引\n",
    "size: 滑动窗口的大小\n",
    "step: 滑动窗口的步幅\n",
    "\"\"\"\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:12.463082Z",
     "end_time": "2023-10-05T23:37:12.480085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 创建窗口分类器模型\n",
    "import torch.nn as nn\n",
    "\n",
    "class WordWindowClassifier(nn.Module):\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding层\n",
    "        self.freeze_embeddings控制是否冻结Embedding层参数\n",
    "        \"\"\"\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embeds.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Hidden层\n",
    "        \"\"\"\n",
    "        full_window_size = 2 * window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Output层\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "        \"\"\"\n",
    "        输出概率\n",
    "        \"\"\"\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        B: 批量大小\n",
    "        L: 两端窗口填充后的句子长度\n",
    "        D: 嵌入维数\n",
    "        S: 窗口总长\n",
    "        H: 隐层维数\n",
    "        输入维数：\n",
    "        形状为(B, L)的索引张量\n",
    "        \"\"\"\n",
    "        B, L = x.size()\n",
    "        \"\"\"\n",
    "        获取滑动窗口\n",
    "        返回形状：(批量大小，窗口数量，窗口总长)\n",
    "        \"\"\"\n",
    "        token_windows = x.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # 维数正确性检验\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding\n",
    "        输入: (批量大小，窗口数量，窗口总长) / (B, L~, S)\n",
    "        输出: (批量大小，窗口数量，窗口总长，嵌入维数) / (B, L~, S, D)\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Reshapeing\n",
    "        (B, L~, S, D) -> (B, L~, S * D)\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "        (B, L~, S * D) -> (B, L~, H)\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        (B, L~, H) -> (B, L~, 1)\n",
    "        \"\"\"\n",
    "        layer_2 = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "        probabilities output\n",
    "        (B, L~, 1) -> (B, L~)\n",
    "        1 probability for 1 window\n",
    "        \"\"\"\n",
    "        output = self.probabilities(layer_2)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:13.120079Z",
     "end_time": "2023-10-05T23:37:13.161079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 准备数据，定义模型、损失函数、优化器\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "loader = DataLoader(data, batch_size=batch_size,shuffle=shuffle,collate_fn=collate_fn)\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\":4,\n",
    "    \"window_size\":2,\n",
    "    \"embed_dim\":25,\n",
    "    \"hidden_dim\":25,\n",
    "    \"freeze_embeddings\":False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    bceloss = nn.BCELoss()\n",
    "    \"\"\"\n",
    "    nn.BCELoss\n",
    "    measures the Binary Cross Entropy between the target and the input probabilities\n",
    "    计算目标和输入概率的二分类交叉熵\n",
    "    l(x, y) = mean(-w_n*[y_n*log(x_n)+(1-y_n)*log(1-x_n)]) / sum(-w_n*[y_n*log(x_n)+(1-y_n)*log(1-x_n)])\n",
    "    y: label\n",
    "    x: probability\n",
    "    \"\"\"\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:13.774669Z",
     "end_time": "2023-10-05T23:37:13.811019Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, loss_function, optimizer):\n",
    "    total_loss = 0\n",
    "    # batch_lengths存放每个样本在两端窗口填充，批量长度一致填充前的单词数\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        batch_loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def train(loader, model, loss_function, optimizer, num_epochs=10000):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loader, model, loss_function, optimizer)\n",
    "        if epoch % 100 == 0: print(epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:14.367018Z",
     "end_time": "2023-10-05T23:37:14.404528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2751699239015579\n",
      "0.2535048946738243\n",
      "0.21989352256059647\n",
      "0.17295077443122864\n",
      "0.14909899234771729\n",
      "0.12179402261972427\n",
      "0.09929980896413326\n",
      "0.08036387152969837\n",
      "0.07015462033450603\n",
      "0.05990997422486544\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loader, model, loss_function, optimizer, num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:15.076835Z",
     "end_time": "2023-10-05T23:37:18.233794Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "([['she', 'comes', 'from', 'paris']], [[0, 0, 0, 1]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型预测\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "test_sentences, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:39.740088Z",
     "end_time": "2023-10-05T23:37:39.760391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['she', 'comes', 'from', 'paris'], [0, 0, 0, 1])]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of tuples\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:37:40.418366Z",
     "end_time": "2023-10-05T23:37:40.434962Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:38:48.521787Z",
     "end_time": "2023-10-05T23:38:48.599784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.2002, 0.2195, 0.1137, 0.9366]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "  outputs = model.forward(test_instance)\n",
    "  print(labels)\n",
    "  print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-05T23:38:54.743982Z",
     "end_time": "2023-10-05T23:38:54.890006Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模块梳理\n",
    "data = list(zip(sentences, labels))\n",
    "sentences:[[word1, word2, ...], ...], labels:[[label1, label2, ...], ...]\n",
    "torch.utils.data.DataLoader(data, batch_size, shuffle, collate_fn)\n",
    "collate_fn is used for data preprocessing and is implemented using partial\n",
    "collate_fn = partial(custom_collate_fn, argument1=value1, argument2=value2)\n",
    "Here, argument1 is window_size, and argument2 is word_to_ix\n",
    "custom_collate_fn returns window-padded and batch-padded sequences of indices and batch-padded labels and the number of words per example given batches.\n",
    "senteces -> indices, \"<pad>\" for padding and \"<unk>\" represents word not in the vocabulary\n",
    "In WindowClassifier model, a dictionary is used to manage hyperparameters, such as window_size, embed_dim, hidden_dim and\n",
    "freeze_embeddings.\n",
    "The dimension conversion include:\n",
    "x.unfold: to get sliding windows, (B, L) -> (B, L~(the number of windows), S(full_window_size))\n",
    "batch-padding ensures that the number of windows is equal for each example.\n",
    "nn.Embedding: (B, L~, S) -> (B, L~, S, D)\n",
    "tensor.view: (B, L~, S, D) -> (B, L~, S * D)\n",
    "nn.Linear: (B, L~, S * D) -> (B, L~, self.hidden_dim)\n",
    "nn.Linear: (B, L~, self.hidden_dim) -> (B, L~, 1)\n",
    "nn.Sigmoid: to get probabilities\n",
    "tensor.view: (B, L~, 1) -> (B, L~)\n",
    "\n",
    "At last, define optimizer, loss_function and training process\n",
    "loss_function: the number of windows is equal to the number of words per example, thus equal to the number of labels.\n",
    "\n",
    "Iteration 0\n",
    "Batched Input:\n",
    "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0],\n",
    "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0]])\n",
    "Batched Labels:\n",
    "tensor([[0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 1, 0]])\n",
    "Batched Lengths:\n",
    "tensor([5, 4])\n",
    "\n",
    "1.Window-padding first and batch-padding second is equivalent to\n",
    "batch-padding first and window-padding second.\n",
    "2.Only batch-padding for labels.\n",
    "So the length of model output is equal to the length of labels for each example in a batch."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
